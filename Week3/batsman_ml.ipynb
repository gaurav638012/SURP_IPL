{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5f7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "207d2703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [ 1.   20.   22.   90.91  1.    2.    0.    8.   18.    1.    2.67 18.\n",
      "  0.    0.    0.    1.  ] 1.0\n",
      "683 [  3.    25.    13.   192.31   0.     2.     1.     0.     0.     0.\n",
      "  -1.    -1.     0.     0.     0.     1.  ] 1.0\n",
      "822 [ 0.    0.    0.   -1.    0.    0.    0.   19.   24.    2.    4.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "908 [ 0.   0.   0.  -1.   0.   0.   0.  10.  24.   2.   2.5 12.   0.   0.\n",
      "  0.   0. ] 1.0\n",
      "1397 [  1.    23.    18.   127.78   1.     2.     1.    13.    16.     1.\n",
      "   4.88  16.     0.     0.     0.     1.  ] 1.0\n",
      "1887 [ 4.   24.   26.   92.31  0.    2.    0.   19.   25.    2.    4.56 12.5\n",
      "  0.    0.    1.    1.  ] 1.0\n",
      "2008 [  0.    24.    14.   171.43   1.     2.     2.    19.    24.     1.\n",
      "   4.75  24.     0.     0.     1.     0.  ] 1.0\n",
      "2042 [  0.  29.  25. 116.   1.   3.   1.  24.  24.   2.   6.  12.   0.   0.\n",
      "   1.   0.] 1.0\n",
      "2236 [ 2.    1.    3.   33.33  1.    0.    0.   22.   24.    2.    5.5  12.\n",
      "  0.    0.    0.    1.  ] 1.0\n",
      "2405 [  1.    11.     6.   183.33   0.     0.     1.    19.    24.     2.\n",
      "   4.75  12.     0.     0.     0.     0.  ] 1.0\n",
      "2497 [  4.  25.  10. 250.   0.   3.   1.  21.  18.   1.   7.  18.   0.   0.\n",
      "   2.   1.] 1.0\n",
      "2564 [ 3.   0.   0.  -1.   0.   0.   0.  18.  24.   2.   4.5 12.   0.   0.\n",
      "  1.   0. ] 1.0\n",
      "3398 [ 1.    0.    0.   -1.    0.    0.    0.    9.   24.    1.    2.25 24.\n",
      "  0.    0.    2.    0.  ] 1.0\n",
      "3490 [  2.   5.   4. 125.   1.   1.   0.  12.  24.   2.   3.  12.   0.   0.\n",
      "   0.   0.] 1.0\n",
      "3537 [ 0.    0.    0.   -1.    0.    0.    0.   17.   24.    2.    4.25 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "4313 [ 0.   25.   27.   92.59  1.    5.    0.    0.    0.    0.   -1.   -1.\n",
      "  0.    0.    1.    1.  ] 1.0\n",
      "4402 [ 0.    0.    0.   -1.    0.    0.    0.   15.   24.    1.    3.75 24.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "4451 [ 3.  0.  0. -1.  0.  0.  0. 16. 24.  2.  4. 12.  0.  0.  1.  0.] 1.0\n",
      "4764 [ 2.    0.    0.   -1.    0.    0.    0.   27.   28.    2.    5.79 14.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "4888 [ 0.    0.    0.   -1.    0.    0.    0.   16.   26.    2.    3.69 13.\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "5278 [ 1.   0.   0.  -1.   0.   0.   0.  22.  24.   2.   5.5 12.   0.   0.\n",
      "  2.   0. ] 1.0\n",
      "5360 [ 0.    0.    0.   -1.    0.    0.    0.   16.   25.    2.    3.84 12.5\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "5919 [ 2.   0.   0.  -1.   0.   0.   0.  14.  24.   2.   3.5 12.   0.   0.\n",
      "  0.   0. ] 1.0\n",
      "6026 [ 8.    3.    5.   60.    1.    0.    0.   10.   27.    2.    2.22 13.5\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "6031 [ 0.    0.    0.   -1.    0.    0.    0.   17.   24.    2.    4.25 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "6057 [ 0.    0.    0.   -1.    0.    0.    0.   23.   24.    2.    5.75 12.\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "6721 [  2.  28.  14. 200.   1.   1.   3.   0.   0.   0.  -1.  -1.   0.   0.\n",
      "   1.   0.] 1.0\n",
      "7015 [  9.   1.   1. 100.   0.   0.   0.   8.  24.   2.   2.  12.   0.   0.\n",
      "   0.   1.] 1.0\n",
      "7062 [ 0.    0.    0.   -1.    0.    0.    0.   23.   24.    2.    5.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "7493 [  6.   18.    6.  300.    0.    4.    0.   22.   24.    1.    5.5  24.\n",
      "   0.    0.    0.    1. ] 1.0\n",
      "7494 [ 3.    0.    0.   -1.    0.    0.    0.   26.   25.    2.    6.24 12.5\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "7512 [  3.    16.    14.   114.29   0.     2.     0.    15.    24.     1.\n",
      "   3.75  24.     0.     0.     0.     0.  ] 1.0\n",
      "7524 [ 8.  0.  0. -1.  1.  0.  0.  8. 24.  2.  2. 12.  0.  0.  0.  1.] 1.0\n",
      "7649 [ 0.    0.    0.   -1.    0.    0.    0.   19.   24.    2.    4.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "7652 [ 0.    0.    0.   -1.    0.    0.    0.   16.   25.    1.    3.84 25.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "7718 [  2.    24.     9.   266.67   0.     2.     2.    27.    12.     0.\n",
      "  13.5   -1.     0.     0.     0.     0.  ] 1.0\n",
      "9082 [ 2.    0.    0.   -1.    0.    0.    0.   20.   26.    2.    4.62 13.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "9118 [ 0.   0.   0.  -1.   0.   0.   0.  18.  24.   2.   4.5 12.   0.   0.\n",
      "  0.   0. ] 1.0\n",
      "9266 [  7.    14.    14.   100.     1.     1.     1.    25.    27.     2.\n",
      "   5.56  13.5    0.     0.     0.     1.  ] 1.0\n",
      "9565 [  6.    17.     4.   425.     0.     1.     2.    24.    25.     2.\n",
      "   5.76  12.5    0.     0.     0.     1.  ] 1.0\n",
      "9573 [ 8.    0.    1.    0.    0.    0.    0.   22.   25.    2.    5.28 12.5\n",
      "  0.    0.    0.    1.  ] 1.0\n",
      "10072 [  2.    20.    11.   181.82   1.     1.     2.    19.    19.     1.\n",
      "   6.    19.     0.     0.     2.     1.  ] 1.0\n",
      "10223 [ 0.  0.  0. -1.  0.  0.  0. 16. 24.  2.  4. 12.  0.  0.  0.  0.] 1.0\n",
      "10399 [ 0.    0.    0.   -1.    0.    0.    0.   19.   27.    2.    4.22 13.5\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "10404 [ 0.    0.    0.   -1.    0.    0.    0.   34.   26.    2.    7.85 13.\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "10458 [ 7.    0.    1.    0.    1.    0.    0.   19.   27.    2.    4.22 13.5\n",
      "  0.    0.    1.    1.  ] 1.0\n",
      "10496 [ 1.    0.    1.    0.    0.    0.    0.    7.   24.    2.    1.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "10497 [ 0.   0.   0.  -1.   0.   0.   0.  18.  24.   2.   4.5 12.   0.   0.\n",
      "  0.   0. ] 1.0\n",
      "11047 [ 1.    0.    0.   -1.    0.    0.    0.   24.   26.    2.    5.54 13.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "11625 [ 0.    0.    0.   -1.    0.    0.    0.   16.   25.    2.    3.84 12.5\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "11679 [ 2.    0.    0.   -1.    0.    0.    0.   19.   24.    2.    4.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "11686 [ 0.    0.    0.   -1.    0.    0.    0.   29.   24.    2.    7.25 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "11726 [ 1.    0.    0.   -1.    0.    0.    0.   25.   27.    2.    5.56 13.5\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "11743 [ 0.  0.  0. -1.  0.  0.  0. 24. 24.  2.  6. 12.  0.  0.  1.  0.] 1.0\n",
      "11802 [ 0.    0.    0.   -1.    0.    0.    0.   17.   27.    2.    3.78 13.5\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "11833 [ 0.  0.  0. -1.  0.  0.  0. 28. 24.  2.  7. 12.  0.  0.  1.  0.] 1.0\n",
      "12313 [ 3.    0.    0.   -1.    0.    0.    0.   30.   23.    2.    7.83 11.5\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "12449 [ 4.    0.    0.   -1.    0.    0.    0.   17.   24.    1.    4.25 24.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "12714 [ 0.   0.   0.  -1.   0.   0.   0.  30.  25.   2.   7.2 12.5  0.   0.\n",
      "  0.   0. ] 1.0\n",
      "12753 [ 0.  0.  0. -1.  0.  0.  0. 10. 12.  2.  5.  6.  0.  0.  2.  0.] 1.0\n",
      "13150 [ 0.    0.    0.   -1.    0.    0.    0.   23.   26.    2.    5.31 13.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "13257 [ 1.  0.  0. -1.  0.  0.  0. 12. 18.  2.  4.  9.  0.  0.  2.  0.] 1.0\n",
      "13284 [ 1.   0.   0.  -1.   0.   0.   0.  26.  24.   2.   6.5 12.   0.   0.\n",
      "  2.   0. ] 1.0\n",
      "13318 [ 0.   0.   0.  -1.   0.   0.   0.  40.  25.   2.   9.6 12.5  0.   0.\n",
      "  0.   0. ] 1.0\n",
      "13406 [ 0.    0.    0.   -1.    0.    0.    0.   19.   25.    1.    4.56 25.\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "13744 [ 0.  0.  0. -1.  0.  0.  0.  8. 24.  2.  2. 12.  0.  0.  0.  0.] 1.0\n",
      "13835 [ 2.   18.   20.   90.    0.    0.    1.   10.   18.    2.    3.33  9.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "13940 [ 0.  0.  0. -1.  0.  0.  0. 24. 18.  2.  8.  9.  0.  0.  0.  0.] 1.0\n",
      "14048 [ 0.   0.   0.  -1.   0.   0.   0.  20.  25.   2.   4.8 12.5  0.   0.\n",
      "  0.   0. ] 1.0\n",
      "14245 [  4.    20.    15.   133.33   0.     1.     1.    13.    12.     2.\n",
      "   6.5    6.     0.     0.     0.     1.  ] 1.0\n",
      "14262 [  0.   9.   6. 150.   1.   0.   1.  12.  24.   1.   3.  24.   0.   0.\n",
      "   0.   0.] 1.0\n",
      "14303 [ 1.    0.    0.   -1.    0.    0.    0.   19.   24.    2.    4.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "14386 [ 0.    0.    0.   -1.    0.    0.    0.   31.   24.    2.    7.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "14389 [ 1.    0.    0.   -1.    0.    0.    0.   19.   24.    2.    4.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "14444 [ 6.   1.   2.  50.   1.   0.   0.  25.  25.   2.   6.  12.5  0.   0.\n",
      "  0.   1. ] 1.0\n",
      "14494 [ 4.    0.    0.   -1.    0.    0.    0.   18.   25.    1.    4.32 25.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "15051 [  1.   21.    8.  262.5   0.    0.    3.    4.    6.    0.    4.   -1.\n",
      "   0.    0.    3.    0. ] 1.0\n",
      "15111 [ 1.    0.    0.   -1.    0.    0.    0.   20.   18.    2.    6.67  9.\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "15497 [ 2.    0.    0.   -1.    0.    0.    0.   15.   24.    1.    3.75 24.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "15922 [  3.    17.    12.   141.67   1.     2.     0.    23.    25.     2.\n",
      "   5.52  12.5    0.     0.     1.     1.  ] 1.0\n",
      "16145 [ 3.   0.   0.  -1.   0.   0.   0.  18.  24.   1.   4.5 24.   0.   0.\n",
      "  0.   0. ] 1.0\n",
      "16153 [ 1.    0.    1.    0.    1.    0.    0.   17.   25.    1.    4.08 25.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "16157 [ 6.    6.    9.   66.67  1.    0.    0.   11.   24.    2.    2.75 12.\n",
      "  0.    0.    1.    1.  ] 1.0\n",
      "16160 [ 1.    0.    0.   -1.    0.    0.    0.   23.   24.    2.    5.75 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "16170 [  0.   15.    8.  187.5   0.    1.    1.   24.   24.    1.    6.   24.\n",
      "   0.    0.    0.    0. ] 1.0\n",
      "16231 [ 1.    0.    0.   -1.    0.    0.    0.    8.   25.    2.    1.92 12.5\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "16496 [ 0.  0.  0. -1.  0.  0.  0. 28. 24.  2.  7. 12.  0.  0.  0.  0.] 1.0\n",
      "16588 [ 0.    0.    0.   -1.    0.    0.    0.   21.   24.    2.    5.25 12.\n",
      "  0.    0.    0.    0.  ] 1.0\n",
      "16657 [ 1.    0.    0.   -1.    0.    0.    0.   21.   24.    2.    5.25 12.\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "16791 [ 1.    0.    0.   -1.    0.    0.    0.    9.   24.    2.    2.25 12.\n",
      "  0.    0.    1.    0.  ] 1.0\n",
      "17051 [ 1.   0.   0.  -1.   0.   0.   0.  25.  25.   2.   6.  12.5  0.   0.\n",
      "  0.   0. ] 1.0\n",
      "17147 [ 0.   0.   0.  -1.   0.   0.   0.  34.  24.   2.   8.5 12.   0.   0.\n",
      "  0.   0. ] 1.0\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "dataset1 = dataset[['Batting Position', 'Runs Scored', 'Balls Faced', 'Batting Strikerate', 'Out/NotOut', '4s', '6s',\n",
    "                   'Runs Given','Balls Bowled', 'Wickets Taken', 'Economy Rate', 'Bowling Strikerate','3 wicket hauls',\n",
    "                    '5 wicket hauls', 'Catches or Runouts as Fielder','Match Result', 'Man of the match']]\n",
    "data = dataset1.to_numpy()\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "for i in range(len(Y)):\n",
    "    if Y[i]>0 and X[i,1]<30 and X[i,9]<3:\n",
    "        print(i, X[i], Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b2bc445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0.5245017869041129, 1.0: 10.703337453646476}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(Y), Y)\n",
    "weights = {0.0: class_weights[0], 1.0: class_weights[1]}\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b20e42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2535f0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,537\n",
      "Trainable params: 9,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape = (16,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f87400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "102/102 [==============================] - 3s 12ms/step - loss: 0.2449 - tp: 561.0000 - fp: 1555.0000 - tn: 10820.0000 - fn: 52.0000 - accuracy: 0.8763 - precision: 0.2651 - recall: 0.9152 - val_loss: 0.3569 - val_tp: 192.0000 - val_fp: 680.0000 - val_tn: 3454.0000 - val_fn: 4.0000 - val_accuracy: 0.8420 - val_precision: 0.2202 - val_recall: 0.9796\n",
      "Epoch 2/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2277 - tp: 575.0000 - fp: 1584.0000 - tn: 10791.0000 - fn: 38.0000 - accuracy: 0.8751 - precision: 0.2663 - recall: 0.9380 - val_loss: 0.2179 - val_tp: 182.0000 - val_fp: 445.0000 - val_tn: 3689.0000 - val_fn: 14.0000 - val_accuracy: 0.8940 - val_precision: 0.2903 - val_recall: 0.9286\n",
      "Epoch 3/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2070 - tp: 579.0000 - fp: 1448.0000 - tn: 10927.0000 - fn: 34.0000 - accuracy: 0.8859 - precision: 0.2856 - recall: 0.9445 - val_loss: 0.3230 - val_tp: 190.0000 - val_fp: 610.0000 - val_tn: 3524.0000 - val_fn: 6.0000 - val_accuracy: 0.8577 - val_precision: 0.2375 - val_recall: 0.9694\n",
      "Epoch 4/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2248 - tp: 575.0000 - fp: 1555.0000 - tn: 10820.0000 - fn: 38.0000 - accuracy: 0.8773 - precision: 0.2700 - recall: 0.9380 - val_loss: 0.3288 - val_tp: 190.0000 - val_fp: 624.0000 - val_tn: 3510.0000 - val_fn: 6.0000 - val_accuracy: 0.8545 - val_precision: 0.2334 - val_recall: 0.9694\n",
      "Epoch 5/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2184 - tp: 577.0000 - fp: 1541.0000 - tn: 10834.0000 - fn: 36.0000 - accuracy: 0.8786 - precision: 0.2724 - recall: 0.9413 - val_loss: 0.2379 - val_tp: 183.0000 - val_fp: 485.0000 - val_tn: 3649.0000 - val_fn: 13.0000 - val_accuracy: 0.8850 - val_precision: 0.2740 - val_recall: 0.9337\n",
      "Epoch 6/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2084 - tp: 586.0000 - fp: 1479.0000 - tn: 10896.0000 - fn: 27.0000 - accuracy: 0.8840 - precision: 0.2838 - recall: 0.9560 - val_loss: 0.2728 - val_tp: 185.0000 - val_fp: 478.0000 - val_tn: 3656.0000 - val_fn: 11.0000 - val_accuracy: 0.8871 - val_precision: 0.2790 - val_recall: 0.9439\n",
      "Epoch 7/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2226 - tp: 580.0000 - fp: 1557.0000 - tn: 10818.0000 - fn: 33.0000 - accuracy: 0.8776 - precision: 0.2714 - recall: 0.9462 - val_loss: 0.3495 - val_tp: 183.0000 - val_fp: 613.0000 - val_tn: 3521.0000 - val_fn: 13.0000 - val_accuracy: 0.8554 - val_precision: 0.2299 - val_recall: 0.9337\n",
      "Epoch 8/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2149 - tp: 578.0000 - fp: 1475.0000 - tn: 10900.0000 - fn: 35.0000 - accuracy: 0.8837 - precision: 0.2815 - recall: 0.9429 - val_loss: 0.2807 - val_tp: 187.0000 - val_fp: 537.0000 - val_tn: 3597.0000 - val_fn: 9.0000 - val_accuracy: 0.8739 - val_precision: 0.2583 - val_recall: 0.9541\n",
      "Epoch 9/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2138 - tp: 579.0000 - fp: 1516.0000 - tn: 10859.0000 - fn: 34.0000 - accuracy: 0.8807 - precision: 0.2764 - recall: 0.9445 - val_loss: 0.2593 - val_tp: 185.0000 - val_fp: 478.0000 - val_tn: 3656.0000 - val_fn: 11.0000 - val_accuracy: 0.8871 - val_precision: 0.2790 - val_recall: 0.9439\n",
      "Epoch 10/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2171 - tp: 583.0000 - fp: 1491.0000 - tn: 10884.0000 - fn: 30.0000 - accuracy: 0.8829 - precision: 0.2811 - recall: 0.9511 - val_loss: 0.2776 - val_tp: 188.0000 - val_fp: 569.0000 - val_tn: 3565.0000 - val_fn: 8.0000 - val_accuracy: 0.8667 - val_precision: 0.2483 - val_recall: 0.9592\n",
      "Epoch 11/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2055 - tp: 582.0000 - fp: 1492.0000 - tn: 10883.0000 - fn: 31.0000 - accuracy: 0.8827 - precision: 0.2806 - recall: 0.9494 - val_loss: 0.1765 - val_tp: 170.0000 - val_fp: 362.0000 - val_tn: 3772.0000 - val_fn: 26.0000 - val_accuracy: 0.9104 - val_precision: 0.3195 - val_recall: 0.8673\n",
      "Epoch 12/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2022 - tp: 583.0000 - fp: 1416.0000 - tn: 10959.0000 - fn: 30.0000 - accuracy: 0.8887 - precision: 0.2916 - recall: 0.9511 - val_loss: 0.2463 - val_tp: 185.0000 - val_fp: 503.0000 - val_tn: 3631.0000 - val_fn: 11.0000 - val_accuracy: 0.8813 - val_precision: 0.2689 - val_recall: 0.9439\n",
      "Epoch 13/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2072 - tp: 580.0000 - fp: 1445.0000 - tn: 10930.0000 - fn: 33.0000 - accuracy: 0.8862 - precision: 0.2864 - recall: 0.9462 - val_loss: 0.2830 - val_tp: 186.0000 - val_fp: 525.0000 - val_tn: 3609.0000 - val_fn: 10.0000 - val_accuracy: 0.8764 - val_precision: 0.2616 - val_recall: 0.9490\n",
      "Epoch 14/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2118 - tp: 586.0000 - fp: 1472.0000 - tn: 10903.0000 - fn: 27.0000 - accuracy: 0.8846 - precision: 0.2847 - recall: 0.9560 - val_loss: 0.3733 - val_tp: 189.0000 - val_fp: 734.0000 - val_tn: 3400.0000 - val_fn: 7.0000 - val_accuracy: 0.8289 - val_precision: 0.2048 - val_recall: 0.9643\n",
      "Epoch 15/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2061 - tp: 579.0000 - fp: 1456.0000 - tn: 10919.0000 - fn: 34.0000 - accuracy: 0.8853 - precision: 0.2845 - recall: 0.9445 - val_loss: 0.2816 - val_tp: 186.0000 - val_fp: 509.0000 - val_tn: 3625.0000 - val_fn: 10.0000 - val_accuracy: 0.8801 - val_precision: 0.2676 - val_recall: 0.9490\n",
      "Epoch 16/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2079 - tp: 582.0000 - fp: 1445.0000 - tn: 10930.0000 - fn: 31.0000 - accuracy: 0.8864 - precision: 0.2871 - recall: 0.9494 - val_loss: 0.2395 - val_tp: 184.0000 - val_fp: 487.0000 - val_tn: 3647.0000 - val_fn: 12.0000 - val_accuracy: 0.8848 - val_precision: 0.2742 - val_recall: 0.9388\n",
      "Epoch 17/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2213 - tp: 583.0000 - fp: 1541.0000 - tn: 10834.0000 - fn: 30.0000 - accuracy: 0.8790 - precision: 0.2745 - recall: 0.9511 - val_loss: 0.2934 - val_tp: 185.0000 - val_fp: 564.0000 - val_tn: 3570.0000 - val_fn: 11.0000 - val_accuracy: 0.8672 - val_precision: 0.2470 - val_recall: 0.9439\n",
      "Epoch 18/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2014 - tp: 585.0000 - fp: 1378.0000 - tn: 10997.0000 - fn: 28.0000 - accuracy: 0.8917 - precision: 0.2980 - recall: 0.9543 - val_loss: 0.2819 - val_tp: 187.0000 - val_fp: 556.0000 - val_tn: 3578.0000 - val_fn: 9.0000 - val_accuracy: 0.8695 - val_precision: 0.2517 - val_recall: 0.9541\n",
      "Epoch 19/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.2013 - tp: 584.0000 - fp: 1435.0000 - tn: 10940.0000 - fn: 29.0000 - accuracy: 0.8873 - precision: 0.2893 - recall: 0.9527 - val_loss: 0.2988 - val_tp: 188.0000 - val_fp: 588.0000 - val_tn: 3546.0000 - val_fn: 8.0000 - val_accuracy: 0.8624 - val_precision: 0.2423 - val_recall: 0.9592\n",
      "Epoch 20/20\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.1953 - tp: 584.0000 - fp: 1428.0000 - tn: 10947.0000 - fn: 29.0000 - accuracy: 0.8878 - precision: 0.2903 - recall: 0.9527 - val_loss: 0.2547 - val_tp: 184.0000 - val_fp: 514.0000 - val_tn: 3620.0000 - val_fn: 12.0000 - val_accuracy: 0.8785 - val_precision: 0.2636 - val_recall: 0.9388\n"
     ]
    }
   ],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=20, batch_size=128, class_weight=weights, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a0a9dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7775978]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[6:7, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2e6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
